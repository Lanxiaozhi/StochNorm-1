{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StochNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from module.backbone import ResNet50_F\n",
    "from module.stoch_norm import StochNorm2d\n",
    "from utils.transforms import get_transforms\n",
    "from utils.tools import AccuracyMeter, TenCropsTest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Pytorch Stochastic Normalization Training')\n",
    "\n",
    "    # train\n",
    "    parser.add_argument('--gpu', default=0, type=int,\n",
    "                        help='GPU num for training')\n",
    "    parser.add_argument('--seed', type=int, default=2020)\n",
    "\n",
    "    parser.add_argument('--batch_size', default=48, type=int)\n",
    "    parser.add_argument('--total_iter', default=9050, type=int)\n",
    "    parser.add_argument('--eval_iter', default=1000, type=int)\n",
    "    parser.add_argument('--save_iter', default=9000, type=int)\n",
    "    parser.add_argument('--print_iter', default=500, type=int)\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument('--data_path', default=\"/path/to/dataset\",\n",
    "                        type=str, help='Path of dataset')\n",
    "    parser.add_argument('--class_num', default=200,\n",
    "                        type=int, help='number of classes')\n",
    "    parser.add_argument('--num_workers', default=2, type=int,\n",
    "                        help='Num of workers used in dataloading')\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument('--lr', default=1e-3, type=float,\n",
    "                        help='Learning rate for training')\n",
    "    parser.add_argument('--gamma', default=0.1, type=float,\n",
    "                        help='Gamma value for learning rate decay')\n",
    "    parser.add_argument('--nesterov', default=True,\n",
    "                        type=bool, help='nesterov momentum')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                        help='Momentum value for optimizer')\n",
    "    parser.add_argument('--weight_decay', default=5e-4,\n",
    "                        type=float, help='Weight decay value for optimizer')\n",
    "\n",
    "    # experiment\n",
    "    parser.add_argument('--root', default='.', type=str,\n",
    "                        help='Root of the experiment')\n",
    "    parser.add_argument('--name', default='StochNorm', type=str,\n",
    "                        help='Name of the experiment')\n",
    "    parser.add_argument('--p', default=0.5, type=float,\n",
    "                        help='Probability for StochNorm layers')\n",
    "    parser.add_argument('--save_dir', default=\"model\",\n",
    "                        type=str, help='Path of saved models')\n",
    "    parser.add_argument('--visual_dir', default=\"visual\",\n",
    "                        type=str, help='Path of tensorboard data for training')\n",
    "\n",
    "    configs = parser.parse_args(args=['--gpu', '3', '--data_path', '/path', '--lr', '1e-4'])\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def str2list(v):\n",
    "    return v.split(',')\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_writer(log_dir):\n",
    "    return SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(configs):\n",
    "    # data augmentation\n",
    "    data_transforms = get_transforms(resize_size=256, crop_size=224)\n",
    "\n",
    "    # build dataset\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        os.path.join(configs.data_path, 'train'),\n",
    "        transform=data_transforms['train'])\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        os.path.join(configs.data_path, 'val'),\n",
    "        transform=data_transforms['val'])\n",
    "    test_datasets = {\n",
    "        'test' + str(i):\n",
    "            datasets.ImageFolder(\n",
    "                os.path.join(configs.data_path, 'test'),\n",
    "                transform=data_transforms[\"test\" + str(i)]\n",
    "        )\n",
    "        for i in range(10)\n",
    "    }\n",
    "\n",
    "    # build dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=True,\n",
    "                              num_workers=configs.num_workers, pin_memory=True)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                            num_workers=configs.num_workers, pin_memory=True)\n",
    "    test_loaders = {\n",
    "        'test' + str(i):\n",
    "            DataLoader(\n",
    "                test_datasets[\"test\" + str(i)],\n",
    "                batch_size=4, shuffle=False, num_workers=configs.num_workers\n",
    "        )\n",
    "        for i in range(10)\n",
    "    }\n",
    "\n",
    "    return train_loader, val_loader, test_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train validataion and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(configs, train_loader, val_loader, test_loaders, net):\n",
    "    train_len = len(train_loader) - 1\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    # different learning rates for different layers\n",
    "    params_list = [{\"params\": filter(lambda p: p.requires_grad, net.f_net.parameters()),},\n",
    "                   {\"params\": filter(lambda p: p.requires_grad, net.c_net.parameters()), \"lr\": configs.lr * 10}]\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(params_list, lr=configs.lr, weight_decay=configs.weight_decay,\n",
    "                                momentum=configs.momentum, nesterov=configs.nesterov)\n",
    "    milestones = [6000]\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones, gamma=configs.gamma)\n",
    "\n",
    "    # check visual path\n",
    "    visual_path = os.path.join(configs.visual_dir, configs.name)\n",
    "    if not os.path.exists(visual_path):\n",
    "        os.makedirs(visual_path)\n",
    "    writer = get_writer(visual_path)\n",
    "\n",
    "    # check model save path\n",
    "    save_path = os.path.join(configs.save_dir, configs.name)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # start training\n",
    "    for iter_num in range(configs.total_iter):\n",
    "        net.train()\n",
    "\n",
    "        if iter_num % train_len == 0:\n",
    "            train_iter = iter(train_loader)\n",
    "\n",
    "        # Data Stage\n",
    "        data_start = time()\n",
    "\n",
    "        train_inputs, train_labels = next(train_iter)\n",
    "        train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
    "\n",
    "        data_duration = time() - data_start\n",
    "\n",
    "        # Calc Stage\n",
    "        calc_start = time()\n",
    "\n",
    "        train_outputs = net(train_inputs)\n",
    "\n",
    "        loss = classifier_loss = nn.CrossEntropyLoss()(train_outputs, train_labels)\n",
    "        writer.add_scalar('loss/classifier_loss', classifier_loss, iter_num)\n",
    "\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        calc_duration = time() - calc_start\n",
    "\n",
    "        if iter_num % configs.eval_iter == 0:\n",
    "            acc_meter = AccuracyMeter(topk=(1,))\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for val_inputs, val_labels in tqdm(val_loader):\n",
    "                    val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n",
    "                    val_outputs = net(val_inputs)\n",
    "                    acc_meter.update(val_outputs, val_labels)\n",
    "                writer.add_scalar('acc/val_acc', acc_meter.avg[1], iter_num)\n",
    "                print(\n",
    "                    \"Iter: {}/{} Val_Acc: {:2f}\".format(\n",
    "                        iter_num, configs.total_iter, acc_meter.avg[1])\n",
    "                )\n",
    "            acc_meter.reset()\n",
    "\n",
    "        if iter_num % configs.save_iter == 0 and iter_num > 0:\n",
    "            test_acc = TenCropsTest(test_loaders, net)\n",
    "            writer.add_scalar('acc/test_acc', test_acc, iter_num)\n",
    "            print(\n",
    "                \"Iter: {}/{} Test_Acc: {:2f}\".format(\n",
    "                    iter_num, configs.total_iter, test_acc)\n",
    "            )\n",
    "            checkpoint = {\n",
    "                'state_dict': net.state_dict(),\n",
    "                'iter': iter_num,\n",
    "                'acc': test_acc,\n",
    "            }\n",
    "            torch.save(checkpoint,\n",
    "                       os.path.join(save_path, '{}.pkl'.format(iter_num)))\n",
    "            print(\"Model Saved.\")\n",
    "\n",
    "        if iter_num % configs.print_iter == 0:\n",
    "            print(\n",
    "                \"Iter: {}/{} Loss: {:2f}, d/c: {}/{}\".format(iter_num, configs.total_iter, loss, data_duration, calc_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    configs = get_configs()\n",
    "    print(configs)\n",
    "    torch.cuda.set_device(configs.gpu)\n",
    "    set_seeds(configs.seed)\n",
    "\n",
    "    train_loader, val_loader, test_loaders = get_data_loader(configs)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.f_net = ResNet50_F(pretrained=True, norm_layer=StochNorm2d)\n",
    "            self.c_net = nn.Linear(self.f_net.output_dim, configs.class_num)\n",
    "            self.c_net.weight.data.normal_(0, 0.01)\n",
    "            self.c_net.bias.data.fill_(0.0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            feature = self.f_net(x)\n",
    "            out = self.c_net(feature)\n",
    "            return out\n",
    "            \n",
    "    net = Net().cuda()\n",
    "\n",
    "    # set StochNorm layers\n",
    "    for module in net.f_net.modules():\n",
    "        if isinstance(module, StochNorm2d):\n",
    "            module.p = configs.p\n",
    "\n",
    "    train(configs, train_loader, val_loader, test_loaders, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 1.1.0\n",
      "TorchVision 0.3.0\n",
      "Namespace(batch_size=48, class_num=200, data_path='/data/finetune/cub200_15', eval_iter=1000, gamma=0.1, gpu=3, lr=0.0001, momentum=0.9, name='StochNorm', nesterov=True, num_workers=2, p=0.5, print_iter=500, root='.', save_dir='model', save_iter=9000, seed=2020, total_iter=9050, visual_dir='visual', weight_decay=0.0005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0/9050 Val_Acc: 0.878099\n",
      "Iter: 0/9050 Loss: 5.308679, d/c: 0.10743975639343262/0.9772005081176758\n",
      "Iter: 500/9050 Loss: 3.478653, d/c: 0.3940012454986572/0.2276926040649414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1000/9050 Val_Acc: 43.891796\n",
      "Iter: 1000/9050 Loss: 2.467999, d/c: 0.3902451992034912/0.21657633781433105\n",
      "Iter: 1500/9050 Loss: 1.655665, d/c: 0.3385763168334961/0.22804570198059082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2000/9050 Val_Acc: 48.053398\n",
      "Iter: 2000/9050 Loss: 1.159652, d/c: 0.4065871238708496/0.21879196166992188\n",
      "Iter: 2500/9050 Loss: 0.989734, d/c: 0.3759171962738037/0.21810555458068848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3000/9050 Val_Acc: 48.371403\n",
      "Iter: 3000/9050 Loss: 0.693430, d/c: 0.41485095024108887/0.2221660614013672\n",
      "Iter: 3500/9050 Loss: 0.938889, d/c: 0.3791813850402832/0.22136783599853516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4000/9050 Val_Acc: 49.022640\n",
      "Iter: 4000/9050 Loss: 0.683620, d/c: 0.3784644603729248/0.21190619468688965\n",
      "Iter: 4500/9050 Loss: 0.544682, d/c: 0.3894190788269043/0.21595501899719238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5000/9050 Val_Acc: 49.511829\n",
      "Iter: 5000/9050 Loss: 0.608874, d/c: 0.3726048469543457/0.22105169296264648\n",
      "Iter: 5500/9050 Loss: 0.484004, d/c: 0.3883345127105713/0.21877503395080566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:23<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 6000/9050 Val_Acc: 49.846058\n",
      "Iter: 6000/9050 Loss: 0.640509, d/c: 0.3887596130371094/0.2111964225769043\n",
      "Iter: 6500/9050 Loss: 0.694746, d/c: 0.4028656482696533/0.2297976016998291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:22<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 7000/9050 Val_Acc: 49.742744\n",
      "Iter: 7000/9050 Loss: 0.392571, d/c: 0.36809611320495605/0.21572065353393555\n",
      "Iter: 7500/9050 Loss: 0.324146, d/c: 0.3683156967163086/0.22026658058166504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:22<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 8000/9050 Val_Acc: 49.680977\n",
      "Iter: 8000/9050 Loss: 0.307792, d/c: 0.3857300281524658/0.2185518741607666\n",
      "Iter: 8500/9050 Loss: 0.244998, d/c: 0.3447427749633789/0.21872878074645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:22<00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 9000/9050 Val_Acc: 50.148884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1449/1449 [02:23<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 9000/9050 Test_Acc: 50.414223\n",
      "Model Saved.\n",
      "Iter: 9000/9050 Loss: 0.321427, d/c: 0.3952596187591553/0.21174001693725586\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch {}\".format(torch.__version__))\n",
    "print(\"TorchVision {}\".format(torchvision.__version__))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.1",
   "language": "python",
   "name": "pytorch-1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
